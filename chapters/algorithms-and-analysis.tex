\chapter{Algorithms and Analysis}

\section{Experimental (Empirical) Analysis}

Experimental (empirical) analysis is the process of evaluating an algorithm's efficiency by designing, implementing, and testing it in practice. This approach empirically measures time complexity (execution time) and space complexity (memory usage) in relation to the size of its inputs.

\section{Theoretical Analysis}

Theoretical analysis is the process of evaluating an algorithm's efficiency by mathematically determining its time complexity and space complexity in relation to input size, without implementation.

\section{Theoretical Analysis Steps}

\begin{enumerate}
  \item Write the algorithm in pseudocode.
  \item Determine the number of primitive operations executed.
  \item Express the algorithm as a function of the input size. f(n)
  \item Express the function in asymptotic notation through asymptotic analysis.
\end{enumerate}

\section{Asymptotic Analysis}

Asymptotic analysis is the process of evaluating an algorithm's efficiency by classifying the asymptotic bound of its time complexity and space complexity functions.

\section{Asymptotic Notation}

There are three primary types of asymptotic notation:

\begin{itemize}
  \item Big-O Notation - Describes an asymptotic upper bound on a function.
    Given functions $f(n)$ and $g(n)$. A function $f(n)$ is in $O(g(n))$ if there exists a positive constant $c$ and $n_0$ such that $0 \le f(n) \le cg(n)$ for all $n \ge n_0$.
  \item Big-Omega Notation - Describes an asymptotic lower bound on a function.
    Given functions $f(n)$ and $g(n)$. A function $f(n)$ is in $\Omega(g(n))$ if there exists a positive constant $c$ and $n_0$ such that $0 \le cg(n) \le f(n)$ for all $n \ge n_0$.
  \item Big-Theta Notation - Describes an asymptotic tight bound on a function.
    Given functions $f(n)$ and $g(n)$. A function $f(n)$ is in $\Theta(g(n))$ if there exists a positive constant $c_1$, $c_2$ and $n_0$ such that $0 \le c_{1}g(n) \le f(n) \le c_{2}g(n)$ for all $n \ge n_0$.
\end{itemize}

\section{Fundamental Functions}
There are eight fundamental functions in asymptotic analysis:

\begin{itemize}
  \item Constant: $1$
  \item Logarithmic: $\log_{2}{n}$
  \item Linear: $n$
  \item N-Log-N: $n\log_{2}{n}$
  \item Quadratic: $n^2$
  \item Cubic: $n^3$
  \item Exponential: $2^n$
  \item Factorial: $n!$
\end{itemize}

\section{Best-Case, Worst-Case, Average-Case}

There are three types of cases:

\begin{itemize}
  \item Worst-case - The minimum time complexity or space complexity an algorithm takes to complete.
  \item Average-case - The average time complexity or space complexity an algorithm takes to complete.
  \item Best-case - The maximum time complexity or space complexity an algorithm takes to complete.
\end{itemize}

\newpage

\section{Problems}

\begin{enumerate}
  \item Discuss the difference between worst-case and best-case behaviour, and describe this difference using one or more concrete examples.
  \item Given two functions $f(n)$ and $g(n)$, discuss the difference between the following statements:
    \begin{itemize}
      \item $f(n) \in O(g(n))$;
      \item $f(n) \in \Omega(g(n))$;
      \item $f(n) \in \Theta(g(n))$.
    \end{itemize}
  \item A polynomial of degree $k$ is a function given by $f(n) = a_{k}n^{k} + a_{k - 1}n^{k - 1} + \cdots + a_{1}n + a_{0}$, where $a_{0}, \cdots, a_{k}$ are constants. 
    Assume $a_{k} > 0$ for the following questions:
    \begin{enumerate}
      \item Give a tight Big-O bound on f(n) using the simplification rules given in the lectures.
      \item Give a tight Big-Omega bound on f(n) using the simplification rules given in the lectures.
      \item Does a Big-Theta bound exist for f(n)? If so, explain why it exists, then provide it.
    \end{enumerate}
  \item Recall that $f(n)$ is $O(g(n))$ ($f$ is bounded from above by $g$) if there exists positive $c$ and $n_0$ such that $f(n) \le cg(n)$ for all $n \ge n_0$. Using this definition, find and prove big-O bounds for the following functions:
    $$3 + 2 \log_{2}{n}, \quad (n + 1)(n - 1), \quad \sqrt{9n^{5} - 5}$$
  \item Using previous question or otherwise, find big-O bounds for these functions (you are not required to prove these):
    $$2^{n}+n^{2}, \quad \log_{2}{2^{n} \cdot n^{2}}$$
  \item In algorithm analysis, we often omit the base of logarithms for convenience. This question will prove this is a mathematically valid thing to do.
    Using the fact that $\log_{a}(x) = \log_{b}(x)/\log_{b}(a)$ for all $a, b > 1$, prove that $\log_{a}(n)$ is $O(\log_{b}(n))$
  \item Show that $f(n) = n$ is not $O(\log_{2}(n))$ by proving no $c$ and $n_0$ can satisfy the definition.
  \item Recall that $f(n)$ is $\Omega(g(n))$ ($f$ is bounded from below by $g$) if there exist $c$ and $n_0$ such that $f(n) \ge cg(n)$ for $n \ge n_0$.
    Prove that for any strictly positive and increasing function is $\Omega(1)$. Why is this not useful in algorithm analysis?
  \item For simple mathematical functions (e.g. polynomial, exponential), it is easy to see that the big-O and big-Omega bounds coincide. However, algorithms often behave in more interesting ways.
    Give an example of a function f(n) where the tightest big-O and big-Omega bounds are not the same. (Hint: consider piecewise functions.)
  \item Let f(n) and g(n) be positive functions of n. Prove that if $f(n) \in O(g(n))$ then $g(n) \in \Omega(f(n))$.
  \item Matt and Kenton are arguing about the performance of their sorting algorithms. Matt claims that his $\Theta(n\log_{2}(n))$-time algorithm is always faster than Kenton’s $\Theta(n^2)$ time algorithm. To settle the issue, they implement and run the two algorithms on many randomly generated data sets. To Matt’s dismay, they find that if $n < 10000$, then Kenton’s $\Theta(n^{2})$-time algorithm actually runs faster, and only when $n \ge 10000$ is the $\Theta(n\log_{2}(n))$-time algorithm better again. Explain why this scenario is possible.
  \item Determine whether the following statements hold for all possible functions f(n), g(n), and h(n). If true, explain why using the mathematical definitions of big-Omega and big-Theta. If false, provide a counterexample.
    \begin{enumerate}
      \item If $f(n)$ is $\Omega(g(n))$ and $g(n)$ is $\Omega(h(n))$, then $f(n)$ is $\Omega(h(n))$.
      \item If $f(n)$ is $\Omega(g(n))$ and $g(n)$ is $\Omega(f(n))$, then $f(n)$ is $\Theta(g(n))$.
    \end{enumerate}
  \item For each of these questions, briefly explain your answer.
    \begin{enumerate}
      \item If I prove that an algorithm takes $O(n^2)$ worst-case time, is it possible that it takes $O(n)$ on some inputs?
      \item If I prove that an algorithm takes $O(n^2)$ worst-case time, is it possible that it takes $O(n)$ on all inputs?
      \item If I prove that an algorithm takes $\Theta(n^2)$ worst-case time, is it possible that it takes $O(n)$ on some inputs?
      \item If I prove that an algorithm takes $\Theta(n^2)$ worst-case time, is it possible that it takes $O(n)$ on all inputs?
    \end{enumerate}
  \item Suppose we have keys in the range $0, \cdots, m - 1$. Each key has an associated element or null value. To store this, we will use an array with $m$ cells and $n$ non-null elements.
    
    For example, the following array has $m = 16$ and $n = 3$:
    $$[1, null, null, null, 2, null, null, null, null, null, null, null, null, null, null, 3]$$
    
    \begin{enumerate}
      \item What is the memory usage of this data structure (in big-O notation)?
      \item Algorithm A executes an $O(\log_{2}(n))$-time computation for each cell in this data structure. What is the worst-case running time (in big-O notation) of algorithm A?
      \item What are some potential disadvantages of this data structure? Consider if we wanted to store elements bigger than integers.
      \item Give an example of an alternative data structure for this purpose. When would this be better or worse than a sparse array?
    \end{enumerate}
  \item Here is a function which searches an array for a particular element.
    
    \begin{algorithm}
    \caption{arrayFind$(x, A, n)$}
    \begin{algorithmic}[1]
    \REQUIRE An element $x$ and an $n$-element array $A$
    \ENSURE The first index $i$ such that $x = A[i]$, or $-1$ if $x$ does not appear in $A$

    \STATE $i \leftarrow 0$
    \WHILE{$i < n$}
      \IF{$x = A[i]$}
        \RETURN $i$
      \ELSE
        \STATE $i \leftarrow i + 1$
      \ENDIF
    \ENDWHILE
    \RETURN $-1$
    \end{algorithmic}
    \end{algorithm}

    This is used within another function, \texttt{matrixFind}, to find an element $x$ within an $n \times n$ matrix $B$.
    \texttt{matrixFind} iterates over the rows of $B$, calling \texttt{arrayFind} (above) on each row until $x$ is found or it has searched all rows.
    \begin{enumerate}
      \item How many primitive operations are required to compute \texttt{arrayFind}(1, [10, 1], 2)?
      \item What are the best-case and worst-case running times of \texttt{arrayFind}? Give an example of $A, n, x$ for each.
      \item What is the worst-case running time of \texttt{matrixFind} in terms of $n$?
      \item What is the worst-case running time of matrixFind in terms of $N$, where $N$ is the total size of $B$?
      \item Considering (c) and (d), would it be correct to say that matrixFind is a linear-time algorithm? Briefly explain why or why not.
    \end{enumerate}
  \item Consider the following algorithm, arraySigma, which performs some operation on an array of integers. Note that it returns a value and also modifies the array
   
    \begin{algorithm}
    \caption{arraySigma$(A, n)$}
    \begin{algorithmic}[1]
    \REQUIRE A non-empty array $A$ of length $n$
    \ENSURE Returns an integer and modifies $A$

    \STATE $i \leftarrow n - 1$
    \WHILE{$i \geq 0$}
        \STATE $j \leftarrow 0$
        \STATE $x \leftarrow 0$
        \WHILE{$j \leq i$}
            \STATE $x \leftarrow x + A[j]$
            \STATE $j \leftarrow j + 1$
        \ENDWHILE
        \STATE $A[i] \leftarrow x$
        \STATE $i \leftarrow i - 1$
    \ENDWHILE
    \RETURN $A[n - 1]$
    \end{algorithmic}
    \end{algorithm}

    \begin{enumerate}
      \item The return value is some simple function of the array items. What is the returned value?
      \item What is the worst-case running time of \texttt{arraySigma} in big-O notation? Similarly, what is the best-case running time in big-Omega notation?
      \item Work through some examples by hand. What is contained in the array A after the function has executed?
      \item With (c) in mind, devise a more efficient algorithm which is functionally identical to \texttt{arraySigma} (i.e. returns the same value and modifies the array in the same way).
    \end{enumerate}
\end{enumerate}

\newpage

\section{Solutions}

\begin{enumerate}
  \item The worst-case is the maximum time complexity or space complexity an algorithm takes to complete and the best-case is the minimum time complexity or space complexity an algorithm takes to complete.
    For example, consider a linear search algorithm through an unordered list. The worst-case would be the case that the target does not exist and the best-case would be the case that the target is the first element.
  \item
    \begin{itemize}
      \item The statement $f(n) \in O(g(n))$ means that the function $f(n)$ has an asymptotic upper bound $g(n)$.
      \item The statement $f(n) \in \Omega(g(n))$ means that the function $f(n)$ has an asymptotic lower bound $g(n)$.
      \item The statement $f(n) \in \Theta(g(n))$ means that the function $f(n)$ has an asymptotic tight bound $g(n)$.
    \end{itemize}
  \item 
    \begin{enumerate}
      \item $O(n^k)$
      \item $\Omega(n^k)$
      \item Yes, the Big-Theta bound for f(n) exists since the Big-O bound for f(n) and the Big-Omega bound for f(n) are the same. $\Theta(n^k)$
    \end{enumerate}
  \item $f(n) = 3 + 2 \log_{2}{n}$ is $O(\log_{2}{n})$.
    \begin{align*}
      f(n) &<= cg(n) \\
      3 + 2 \log_{2}{n} &<= 3 \log_{2}{n} + 2 \log_{2}{n} \\
      3 + 2 \log_{2}{n} &<= 5 \log_{2}{n} \\ 
      c = 5, & \quad n_0 = 2 \quad \square
    \end{align*}
    $f(n) = (n + 1)(n - 1)$ is $O(n^{2})$
    \begin{align*}
      f(n) &<= cg(n) \\
      (n + 1)(n - 1) &<= n^2 \\
      n^2 - 1 &<= n^2 \\
      c = 1, & \quad n_0 = 1 \quad \square
    \end{align*}
    $f(n) = \sqrt{9n^{5}-5}$ is $O(\sqrt{n^{5}})$
    \begin{align*}
      f(n) &<= cg(n) \\
      \sqrt{9n^5-5} &<= \sqrt{9n^5} \\
      \sqrt{9n^5-5} &<= 3\sqrt{n^5} \\
      c = 3, &\quad n_0 = 1 \quad \square
    \end{align*}
  \item $2^n + n^2$ is $O(2^n)$
    \begin{align*}
      log_{2}(2^{n} \cdot n^{2}) &= \log_{2}{2^{n}} + \log_{2}{n^{2}} \\
      &= n + 2
    \end{align*}
    $\log_{2}(2^{n} \cdot n^{2})$ is $O(n)$
  \item $f(n) = log_{a}(n)$ is $O(log_{b}(n))$
    \begin{align*}
      f(n) &<= cg(n) \\
      log_{a}n = \log_{b}(n)/\log_{b}(a) &<= \log_{b}(n) \\ 
      c = 1, &\quad n_0 = b \quad \square
    \end{align*}
  \item $f(n) = n$ is not in $O(\log_{2}(n))$
    \begin{align*}
      f(n) &<= cg(n) \\
      n &<= c\log_{2}(n) \\
      \frac{n}{\log_2{n}} &<= c
    \end{align*}
    There does not exist a $c$ and $n_0$.
  \item
    \begin{align*}
      f(n) &\ge cg(n) \\
      f(n) &\ge f(1) \\
      c = 1, &\quad n_0 = 1 \quad \square
    \end{align*}
    It is not useful because stating that an algorithm is $\Omega(1)$ is essentially equivalent to saying that any algorithm requires at least one operation to run. While this is true, it provides no meaningful information about the algorithm’s time complexity or space complexity. In other words, the bound $\Omega(n)$ is trivial, since it holds for virtually all algorithms, and therefore does not help us distinguish between different time complexities.
\end{enumerate}
